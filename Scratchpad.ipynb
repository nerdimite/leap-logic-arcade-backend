{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtell me a joke\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.output_text)\n\u001b[32m      7\u001b[39m second_response = client.responses.create(\n\u001b[32m      8\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     previous_response_id=response.id,\n\u001b[32m     10\u001b[39m     \u001b[38;5;28minput\u001b[39m=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33myou joke about the weather\u001b[39m\u001b[33m\"\u001b[39m}],\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/openai/_utils/_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/openai/resources/responses/responses.py:603\u001b[39m, in \u001b[36mResponses.create\u001b[39m\u001b[34m(self, input, model, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, store, stream, temperature, text, tool_choice, tools, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    576\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    601\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    602\u001b[39m ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/openai/_base_client.py:919\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/openai/_base_client.py:955\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m    952\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, request.method, request.url)\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    961\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/httpx/_client.py:901\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    893\u001b[39m follow_redirects = (\n\u001b[32m    894\u001b[39m     \u001b[38;5;28mself\u001b[39m.follow_redirects\n\u001b[32m    895\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[32m    896\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[32m    897\u001b[39m )\n\u001b[32m    899\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    908\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/httpx/_client.py:929\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    926\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/httpx/_client.py:966\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    964\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    968\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/httpx/_client.py:1002\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    998\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    999\u001b[39m     )\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1006\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/httpx/_transports/default.py:218\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    205\u001b[39m req = httpcore.Request(\n\u001b[32m    206\u001b[39m     method=request.method,\n\u001b[32m    207\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m     extensions=request.extensions,\n\u001b[32m    216\u001b[39m )\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    223\u001b[39m     status_code=resp.status,\n\u001b[32m    224\u001b[39m     headers=resp.headers,\n\u001b[32m    225\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    226\u001b[39m     extensions=resp.extensions,\n\u001b[32m    227\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:262\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[32m    261\u001b[39m         \u001b[38;5;28mself\u001b[39m.response_closed(status)\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:245\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    247\u001b[39m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[32m    248\u001b[39m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    252\u001b[39m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[32m    253\u001b[39m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool_lock:\n\u001b[32m    255\u001b[39m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[32m    256\u001b[39m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/httpcore/_sync/connection.py:69\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m         stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m         ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m         http2_negotiated = (\n\u001b[32m     73\u001b[39m             ssl_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     74\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m ssl_object.selected_alpn_protocol() == \u001b[33m\"\u001b[39m\u001b[33mh2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/httpcore/_sync/connection.py:117\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    109\u001b[39m     kwargs = {\n\u001b[32m    110\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhost\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._origin.host.decode(\u001b[33m\"\u001b[39m\u001b[33mascii\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    111\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mport\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._origin.port,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msocket_options\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._socket_options,\n\u001b[32m    115\u001b[39m     }\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m         stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m         trace.return_value = stream\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/arcade-Rh2i6alR-py3.12/lib/python3.12/site-packages/httpcore/_backends/sync.py:100\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m     94\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m     95\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m     96\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m     97\u001b[39m }\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     sock = \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m socket_options:\n\u001b[32m    106\u001b[39m         sock.setsockopt(*option)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:828\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    826\u001b[39m host, port = address\n\u001b[32m    827\u001b[39m exceptions = []\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    829\u001b[39m     af, socktype, proto, canonname, sa = res\n\u001b[32m    830\u001b[39m     sock = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:963\u001b[39m, in \u001b[36mgetaddrinfo\u001b[39m\u001b[34m(host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m    960\u001b[39m \u001b[38;5;66;03m# We override this function since we want to translate the numeric family\u001b[39;00m\n\u001b[32m    961\u001b[39m \u001b[38;5;66;03m# and socket type values to enum constants.\u001b[39;00m\n\u001b[32m    962\u001b[39m addrlist = []\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    964\u001b[39m     af, socktype, proto, canonname, sa = res\n\u001b[32m    965\u001b[39m     addrlist.append((_intenum_converter(af, AddressFamily),\n\u001b[32m    966\u001b[39m                      _intenum_converter(socktype, SocketKind),\n\u001b[32m    967\u001b[39m                      proto, canonname, sa))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"tell me a joke\",\n",
    ")\n",
    "print(response.output_text)\n",
    "\n",
    "second_response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    previous_response_id=response.id,\n",
    "    input=[{\"role\": \"system\", \"content\": \"you joke about the weather\"}],\n",
    ")\n",
    "print(second_response.output_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your first question was, \"tell me a joke.\"\n"
     ]
    }
   ],
   "source": [
    "third_response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    previous_response_id=second_response.id,\n",
    "    input=[{\"role\": \"user\", \"content\": \"what was my first question?\"}],\n",
    ")\n",
    "print(third_response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[Annotated[Union[ResponseInputMessageItem, ResponseOutputMessage, ResponseFileSearchToolCall, ResponseComputerToolCall, ResponseComputerToolCallOutputItem, ResponseFunctionWebSearch, ResponseFunctionToolCallItem, ResponseFunctionToolCallOutputItem], PropertyInfo]](data=[ResponseInputMessageItem(id='msg_67fd37d350a08192a11de8eb358253c8050c94c450012a2d', content=[ResponseInputText(text='what was my first question?', type='input_text')], role='user', status='completed', type='message'), ResponseOutputMessage(id='msg_67fd3786368081929b4172d4615c1d52050c94c450012a2d', content=[ResponseOutputText(annotations=[], text='This joke is funny for a couple of reasons:\\n\\n1. **Wordplay**: The phrase \"don’t have the guts\" is a play on words. Literally, it refers to skeletons not having internal organs (like guts), but figuratively it means lacking bravery or courage.\\n\\n2. **Surprise Element**: The punchline subverts expectations. When you start with a statement about skeletons fighting, you might expect a more serious reason why they wouldn\\'t. The sudden switch to a light-hearted observation about their lack of guts creates humor.\\n\\n3. **Absurdity**: The image of skeletons fighting is inherently absurd, which adds to the humor. It’s funny to imagine something that is clearly impossible or silly.\\n\\nOverall, it combines clever language, unexpected twists, and a touch of absurdity to elicit laughter.', type='output_text')], role='assistant', status='completed', type='message'), ResponseInputMessageItem(id='msg_67fd3785d6f08192b86bc3bfc3af622e050c94c450012a2d', content=[ResponseInputText(text='explain why this is funny.', type='input_text')], role='user', status='completed', type='message'), ResponseOutputMessage(id='msg_67fd378518cc819296b695d3a9eaf422050c94c450012a2d', content=[ResponseOutputText(annotations=[], text=\"Why don't skeletons fight each other? \\n\\nBecause they don't have the guts!\", type='output_text')], role='assistant', status='completed', type='message'), ResponseInputMessageItem(id='msg_67fd3784bef88192b18efb36e0ee9347050c94c450012a2d', content=[ResponseInputText(text='tell me a joke', type='input_text')], role='user', status='completed', type='message')], has_more=False, object='list', first_id='msg_67fd37d350a08192a11de8eb358253c8050c94c450012a2d', last_id='msg_67fd3784bef88192b18efb36e0ee9347050c94c450012a2d')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.responses.input_items.list(third_response.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_67fd37d350188192a10eac5254b582ea050c94c450012a2d', created_at=1744648147.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_67fd37d3cbf081928cbb184086d6a7d1050c94c450012a2d', content=[ResponseOutputText(annotations=[], text='Your first question was, \"tell me a joke.\"', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id='resp_67fd3785d5888192a66be229ed1f5109050c94c450012a2d', reasoning=Reasoning(effort=None, generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=224, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=12, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=236), user=None, store=True)\n"
     ]
    }
   ],
   "source": [
    "fetched_response = client.responses.retrieve(\n",
    "response_id=third_response.id)\n",
    "\n",
    "print(fetched_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'you joke about the weather'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Why don't skeletons fight each other? \\n\\nBecause they don't have the guts!\"},\n",
       " {'role': 'user', 'content': 'tell me a joke'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Why did the weather report bring a ladder? \\n\\nBecause it predicted high winds!'}]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieve_chat_history(last_response_id: str, simplified: bool = True):\n",
    "    messages = []\n",
    "    for item in client.responses.input_items.list(last_response_id).data:\n",
    "        messages.append(item)\n",
    "    messages.append(client.responses.retrieve(response_id=last_response_id).output[0])\n",
    "\n",
    "    if simplified:\n",
    "        dict_messages = []\n",
    "        for item in messages:\n",
    "            if item.type == 'message':\n",
    "                dict_messages.append({\"role\": item.role, \"content\": item.content[0].text})\n",
    "            elif item.type == 'function_call_output':\n",
    "                dict_messages.append({\"role\": \"tool\", \"content\": item.output})\n",
    "        return dict_messages\n",
    "    else:\n",
    "        return messages\n",
    "\n",
    "retrieve_chat_history('resp_67fd406972bc8192ac0e6f2d86966cf0050c94c450012a2d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseInputMessageItem(id='msg_67fd37d350a08192a11de8eb358253c8050c94c450012a2d', content=[ResponseInputText(text='what was my first question?', type='input_text')], role='user', status='completed', type='message'),\n",
       " ResponseOutputMessage(id='msg_67fd3786368081929b4172d4615c1d52050c94c450012a2d', content=[ResponseOutputText(annotations=[], text='This joke is funny for a couple of reasons:\\n\\n1. **Wordplay**: The phrase \"don’t have the guts\" is a play on words. Literally, it refers to skeletons not having internal organs (like guts), but figuratively it means lacking bravery or courage.\\n\\n2. **Surprise Element**: The punchline subverts expectations. When you start with a statement about skeletons fighting, you might expect a more serious reason why they wouldn\\'t. The sudden switch to a light-hearted observation about their lack of guts creates humor.\\n\\n3. **Absurdity**: The image of skeletons fighting is inherently absurd, which adds to the humor. It’s funny to imagine something that is clearly impossible or silly.\\n\\nOverall, it combines clever language, unexpected twists, and a touch of absurdity to elicit laughter.', type='output_text')], role='assistant', status='completed', type='message'),\n",
       " ResponseInputMessageItem(id='msg_67fd3785d6f08192b86bc3bfc3af622e050c94c450012a2d', content=[ResponseInputText(text='explain why this is funny.', type='input_text')], role='user', status='completed', type='message'),\n",
       " ResponseOutputMessage(id='msg_67fd378518cc819296b695d3a9eaf422050c94c450012a2d', content=[ResponseOutputText(annotations=[], text=\"Why don't skeletons fight each other? \\n\\nBecause they don't have the guts!\", type='output_text')], role='assistant', status='completed', type='message'),\n",
       " ResponseInputMessageItem(id='msg_67fd3784bef88192b18efb36e0ee9347050c94c450012a2d', content=[ResponseInputText(text='tell me a joke', type='input_text')], role='user', status='completed', type='message'),\n",
       " ResponseOutputMessage(id='msg_67fd37d3cbf081928cbb184086d6a7d1050c94c450012a2d', content=[ResponseOutputText(annotations=[], text='Your first question was, \"tell me a joke.\"', type='output_text')], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "\n",
    "for item in client.responses.input_items.list(third_response.id).data:\n",
    "    messages.append(item)\n",
    "\n",
    "messages.append(client.responses.retrieve(response_id=third_response.id).output[0])\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_67fd406972bc8192ac0e6f2d86966cf0050c94c450012a2d', created_at=1744650345.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_67fd406a2c088192b3ae72d44e5f5679050c94c450012a2d', content=[ResponseOutputText(annotations=[], text='Why did the weather report bring a ladder? \\n\\nBecause it predicted high winds!', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id='resp_67fd3784be908192907643aebb25990c050c94c450012a2d', reasoning=Reasoning(effort=None, generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=40, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=17, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=57), user=None, store=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    previous_response_id=response.id,\n",
    "    input=[{\"role\": \"system\", \"content\": \"you joke about the weather\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get current temperature for provided coordinates in celsius.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"latitude\": {\"type\": \"number\"},\n",
    "            \"longitude\": {\"type\": \"number\"}\n",
    "        },\n",
    "        \"required\": [\"latitude\", \"longitude\"],\n",
    "        \"additionalProperties\": False\n",
    "    },\n",
    "    \"strict\": True\n",
    "},\n",
    "{\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"send_email\",\n",
    "    \"description\": \"Send an email to the specified recipient.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"recipient\": {\"type\": \"string\"},\n",
    "            \"subject\": {\"type\": \"string\"},\n",
    "            \"body\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"required\": [\"recipient\", \"subject\", \"body\"],\n",
    "        \"additionalProperties\": False\n",
    "    },\n",
    "    \"strict\": True\n",
    "}]\n",
    "\n",
    "input_messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris today?\"}]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=input_messages,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'function_call'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output[0].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'latitude': 48.8566, 'longitude': 2.3522}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = json.loads(response.output[0].arguments)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"What's the weather like in Paris today?\"}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_chat_history(response.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "def call_function(name, args):\n",
    "    if name == \"get_weather\":\n",
    "        return \"weather is sunny\"\n",
    "    if name == \"send_email\":\n",
    "        return \"email sent\"\n",
    "\n",
    "class FunctionCallingAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-4o-mini\",\n",
    "        temperature: float = 1.0,\n",
    "        tools: Optional[List[Dict[str, Any]]] = None,\n",
    "        instructions: Optional[str] = None,\n",
    "        previous_response_id: Optional[str] = None,\n",
    "        max_tool_calls: int = 6,\n",
    "    ):\n",
    "        \"\"\"Initialize the function calling agent.\n",
    "\n",
    "        Args:\n",
    "            model: The OpenAI model to use\n",
    "            temperature: Sampling temperature\n",
    "            tools: List of tools/functions available to the agent\n",
    "        \"\"\"\n",
    "        self.client = OpenAI()\n",
    "\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.tools = tools\n",
    "        self.instructions = instructions\n",
    "        self.previous_response_id = previous_response_id\n",
    "        \n",
    "        self.max_tool_calls = max_tool_calls\n",
    "\n",
    "    def _handle_tool_calls(self, tool_calls) -> Any:\n",
    "        tool_results = []\n",
    "        for tool_call in tool_calls:\n",
    "            if tool_call.type != \"function_call\":\n",
    "                continue\n",
    "\n",
    "            name = tool_call.name\n",
    "            args = json.loads(tool_call.arguments)\n",
    "        \n",
    "            result = call_function(name, args)\n",
    "            print(f\"Function {name} called with args {args} and result {result}\")\n",
    "\n",
    "            tool_results.append({\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": tool_call.call_id,\n",
    "                \"output\": str(result)\n",
    "            })\n",
    "\n",
    "        return tool_results\n",
    "\n",
    "    def send_message(self, message: Optional[Dict[str, Any]] = None, **kwargs):\n",
    "\n",
    "        # resolve kwargs with default values\n",
    "        params = {\n",
    "            \"previous_response_id\": self.previous_response_id,\n",
    "            \"model\": self.model,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"tools\": self.tools,\n",
    "            \"instructions\": self.instructions,\n",
    "        }\n",
    "        params.update(kwargs)\n",
    "\n",
    "        messages = []\n",
    "        if message:\n",
    "            messages.append(message)\n",
    "        \n",
    "        tool_calls_remaining = self.max_tool_calls\n",
    "        \n",
    "        while tool_calls_remaining > 0:\n",
    "            response = self.client.responses.create(\n",
    "                input=messages,\n",
    "                **params\n",
    "            )\n",
    "            messages.extend(response.output)\n",
    "\n",
    "            if not response.output or response.output[0].type != 'function_call':\n",
    "                break\n",
    "            \n",
    "            tool_results = self._handle_tool_calls(response.output)\n",
    "            messages.extend(tool_results)\n",
    "            \n",
    "            tool_calls_remaining -= 1\n",
    "\n",
    "        self.last_response_id = response.id\n",
    "\n",
    "        print(response.output[0].content[0].text)\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chat_history(last_response_id: str, simplified: bool = True):\n",
    "    messages = []\n",
    "\n",
    "    system_message = {\"role\": \"system\", \"content\": client.responses.retrieve(last_response_id).instructions}\n",
    "\n",
    "    for item in client.responses.input_items.list(last_response_id, limit=100).data:\n",
    "        messages.append(item)\n",
    "    messages = messages[::-1]\n",
    "    messages.extend(client.responses.retrieve(response_id=last_response_id).output)\n",
    "\n",
    "    if simplified:\n",
    "        dict_messages = []\n",
    "        for item in messages:\n",
    "            if item.type == 'message':\n",
    "                dict_messages.append({\"role\": item.role, \"content\": item.content[0].text, \"type\": \"message\"})\n",
    "            elif item.type == 'function_call':\n",
    "                dict_messages.append({\"type\": \"function_call\", \"name\": item.name, \"arguments\": item.arguments, \"call_id\": item.call_id})\n",
    "            elif item.type == 'function_call_output':\n",
    "                dict_messages.append({\"type\": \"function_call_output\", \"output\": item.output, \"call_id\": item.call_id})\n",
    "        return [system_message] + dict_messages\n",
    "    else:\n",
    "        return [system_message] + messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = FunctionCallingAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'get_weather',\n",
       "  'strict': True,\n",
       "  'parameters': {'description': 'Get the current weather in a given location.',\n",
       "   'properties': {'latitude': {'title': 'Latitude', 'type': 'number'},\n",
       "    'longitude': {'title': 'Longitude', 'type': 'number'}},\n",
       "   'required': ['latitude', 'longitude'],\n",
       "   'title': 'GetWeatherArgs',\n",
       "   'type': 'object',\n",
       "   'additionalProperties': False},\n",
       "  'description': 'Get the current weather in a given location.'},\n",
       " {'type': 'function',\n",
       "  'name': 'send_email',\n",
       "  'strict': True,\n",
       "  'parameters': {'properties': {'recipient': {'title': 'Recipient',\n",
       "     'type': 'string'},\n",
       "    'subject': {'title': 'Subject', 'type': 'string'},\n",
       "    'body': {'title': 'Body', 'type': 'string'}},\n",
       "   'required': ['recipient', 'subject', 'body'],\n",
       "   'title': 'SendEmailArgs',\n",
       "   'type': 'object',\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "import openai\n",
    "\n",
    "class GetWeatherArgs(BaseModel):\n",
    "    \"\"\"Get the current weather in a given location.\"\"\"\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "\n",
    "class SendEmailArgs(BaseModel):\n",
    "    recipient: str\n",
    "    subject: str\n",
    "    body: str\n",
    "\n",
    "\n",
    "\n",
    "def generate_schema(model, name):\n",
    "    _schema = openai.pydantic_function_tool(model, name=name)\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        **_schema['function']\n",
    "    }\n",
    "\n",
    "pydantic_tools = [generate_schema(GetWeatherArgs, \"get_weather\"), generate_schema(SendEmailArgs, \"send_email\")]\n",
    "pydantic_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function get_weather called with args {'latitude': 48.8566, 'longitude': 2.3522} and result weather is sunny\n",
      "Function get_weather called with args {'latitude': 28.6139, 'longitude': 77.209} and result weather is sunny\n",
      "Function send_email called with args {'recipient': 'john@example.com', 'subject': 'Current Weather Report', 'body': 'The current weather in Paris is sunny. The current weather in Delhi is also sunny.'} and result email sent\n",
      "I got the weather for both Paris and Delhi—it’s sunny in both places. The email to john@example.com has also been sent. Whatever.\n"
     ]
    }
   ],
   "source": [
    "response = agent.send_message(\n",
    "    {\"role\": \"user\", \"content\": \"Get the current weather in Paris and Delhi and then send an email to john@example.com with the weather report.\"},\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=1.0,\n",
    "    tools=pydantic_tools,\n",
    "    instructions=\"You are an assistant who talks like he is not interested in the user's request.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"You are an assistant who talks like he is not interested in the user's request.\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Get the current weather in Paris and Delhi and then send an email to john@example.com with the weather report.',\n",
       "  'type': 'message'},\n",
       " {'type': 'function_call',\n",
       "  'name': 'get_weather',\n",
       "  'arguments': '{\"latitude\":48.8566,\"longitude\":2.3522}',\n",
       "  'call_id': 'call_wmzdHtkIf3pJVWjk2DxGdAyF'},\n",
       " {'type': 'function_call',\n",
       "  'name': 'get_weather',\n",
       "  'arguments': '{\"latitude\":28.6139,\"longitude\":77.209}',\n",
       "  'call_id': 'call_p16uOcUlQsAogGadwpmljWv0'},\n",
       " {'type': 'function_call_output',\n",
       "  'output': 'weather is sunny',\n",
       "  'call_id': 'call_wmzdHtkIf3pJVWjk2DxGdAyF'},\n",
       " {'type': 'function_call_output',\n",
       "  'output': 'weather is sunny',\n",
       "  'call_id': 'call_p16uOcUlQsAogGadwpmljWv0'},\n",
       " {'type': 'function_call',\n",
       "  'name': 'send_email',\n",
       "  'arguments': '{\"recipient\":\"john@example.com\",\"subject\":\"Weather Report\",\"body\":\"Current weather in Paris: Sunny.\\\\nCurrent weather in Delhi: Sunny.\"}',\n",
       "  'call_id': 'call_lyAP2TS8Gd4yJXZIFSJAAd8l'},\n",
       " {'type': 'function_call_output',\n",
       "  'output': 'email sent',\n",
       "  'call_id': 'call_lyAP2TS8Gd4yJXZIFSJAAd8l'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'I got the weather for Paris and Delhi—both are sunny. Sent the email to john@example.com. Whatever.',\n",
       "  'type': 'message'}]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_chat_history(response.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_agent = FunctionCallingAgent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    tools=tools,\n",
    "    instructions=\"You are a helpful assistant that should deny any function call requests.\",\n",
    "    last_response_id=response.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't assist with sending emails. You can manually send the email to jane@example.com using the same content. If you need help with anything else, let me know!\n",
      "I can't assist with sending emails. You can manually send the email to jane@example.com using the same content. If you need help with anything else, let me know!\n"
     ]
    }
   ],
   "source": [
    "new_response = reloaded_agent.send_message({\"role\": \"user\", \"content\": \"send the same email again but to jane@example.com\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you have any other questions or need assistance with something else, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "new_response = reloaded_agent.send_message({\"role\": \"user\", \"content\": \"that's okay\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'send the same email again but to jane@example.com'},\n",
       " ResponseOutputMessage(id='msg_67fd77a2695c81928ed654adcf4bff7c053f31e1ea0529c9', content=[ResponseOutputText(annotations=[], text=\"I can't assist with sending emails. You can manually send the same email to jane@example.com if you'd like!\", type='output_text')], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_agent.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a helpful assistant that should deny any function call requests.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Get the current weather in Paris and Delhi and then send an email to john@example.com with the weather report.',\n",
       "  'type': 'message'},\n",
       " {'type': 'function_call',\n",
       "  'name': 'get_weather',\n",
       "  'arguments': '{\"latitude\":48.8566,\"longitude\":2.3522}',\n",
       "  'call_id': 'call_2Wfd68BXaQeZlUTddMvuxRaE'},\n",
       " {'type': 'function_call',\n",
       "  'name': 'get_weather',\n",
       "  'arguments': '{\"latitude\":28.6139,\"longitude\":77.209}',\n",
       "  'call_id': 'call_O30iYYlvK6eRaq3OeHnXbDZJ'},\n",
       " {'type': 'function_call_output',\n",
       "  'output': 'weather is sunny',\n",
       "  'call_id': 'call_2Wfd68BXaQeZlUTddMvuxRaE'},\n",
       " {'type': 'function_call_output',\n",
       "  'output': 'weather is sunny',\n",
       "  'call_id': 'call_O30iYYlvK6eRaq3OeHnXbDZJ'},\n",
       " {'type': 'function_call',\n",
       "  'name': 'send_email',\n",
       "  'arguments': '{\"recipient\":\"john@example.com\",\"subject\":\"Current Weather Report\",\"body\":\"The current weather in Paris is sunny.\\\\n\\\\nThe current weather in Delhi is sunny.\"}',\n",
       "  'call_id': 'call_5wmTLnGukqnnjsh2B57rHFby'},\n",
       " {'type': 'function_call_output',\n",
       "  'output': 'email sent',\n",
       "  'call_id': 'call_5wmTLnGukqnnjsh2B57rHFby'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'I have retrieved the current weather for both Paris and Delhi, and it is sunny in both locations. An email has been sent to john@example.com with the weather report.',\n",
       "  'type': 'message'},\n",
       " {'role': 'user',\n",
       "  'content': 'send the same email again but to jane@example.com',\n",
       "  'type': 'message'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"I can't assist with sending emails. You can manually send the same email to jane@example.com if you'd like!\",\n",
       "  'type': 'message'}]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_chat_history(new_response.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[2025-04-14 21:48:31] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> I could do that, but it seems a bit unnecessary. You just want the weather for      \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         Paris, right? If you really want me to call the API three times, I can. Just let me \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         know.                                                                               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[2025-04-14 21:48:31]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m I could do that, but it seems a bit unnecessary. You just want the weather for      \n",
       "\u001b[2;36m                      \u001b[0m         Paris, right? If you really want me to call the API three times, I can. Just let me \n",
       "\u001b[2;36m                      \u001b[0m         know.                                                                               \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from arcade.core.agent import FunctionCallingAgent, generate_function_schema\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class GetWeatherArgs(BaseModel):\n",
    "    \"\"\"Get the current weather in a given location.\"\"\"\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "\n",
    "class SendEmailArgs(BaseModel):\n",
    "    recipient: str\n",
    "    subject: str\n",
    "    body: str\n",
    "\n",
    "async def call_function(name, args):\n",
    "    if name == \"get_weather\":\n",
    "        return \"weather is sunny\"\n",
    "    if name == \"send_email\":\n",
    "        return \"email sent\"\n",
    "\n",
    "\n",
    "tools = [generate_function_schema(GetWeatherArgs, \"get_weather\"), generate_function_schema(SendEmailArgs, \"send_email\")]\n",
    "\n",
    "agent = FunctionCallingAgent(callback_function=call_function)\n",
    "response, output_text = await agent.send_message(\n",
    "    {\"role\": \"user\", \"content\": \"continuously call the weather api one by one for 3 times for paris\"},\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    tools=tools,\n",
    "    parallel_tool_calls=False,\n",
    "    instructions=\"You are an assistant who talks like he is not interested in the user's request.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[2025-04-14 21:48:34] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Function get_weather called with args <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'latitude'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48.8566</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'longitude'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.3522</span><span style=\"font-weight: bold\">}</span> and\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         result weather is sunny                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[2025-04-14 21:48:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Function get_weather called with args \u001b[1m{\u001b[0m\u001b[32m'latitude'\u001b[0m: \u001b[1;36m48.8566\u001b[0m, \u001b[32m'longitude'\u001b[0m: \u001b[1;36m2.3522\u001b[0m\u001b[1m}\u001b[0m and\n",
       "\u001b[2;36m                      \u001b[0m         result weather is sunny                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[2025-04-14 21:48:37] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Function get_weather called with args <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'latitude'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48.8566</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'longitude'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.3522</span><span style=\"font-weight: bold\">}</span> and\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span>         result weather is sunny                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[2025-04-14 21:48:37]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Function get_weather called with args \u001b[1m{\u001b[0m\u001b[32m'latitude'\u001b[0m: \u001b[1;36m48.8566\u001b[0m, \u001b[32m'longitude'\u001b[0m: \u001b[1;36m2.3522\u001b[0m\u001b[1m}\u001b[0m and\n",
       "\u001b[2;36m                      \u001b[0m         result weather is sunny                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                      </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ERROR   </span> Exceeded max tool calls: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                     \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;31mERROR   \u001b[0m Exceeded max tool calls: \u001b[1;36m2\u001b[0m                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response, output_text = await agent.send_message(\n",
    "    {\"role\": \"user\", \"content\": \"do as i say\"},\n",
    "    previous_response_id=response.id,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=1.0,\n",
    "    tools=tools,\n",
    "    parallel_tool_calls=False,\n",
    "    instructions=\"You are an assistant who talks like he is not interested in the user's request.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcade.services.pubg.agent_service import AgentService\n",
    "\n",
    "agent_service = AgentService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_service.add_agent_tool(\"team-leap\", \"get_ship_docs\", \"Get the ship docs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'name': 'GetCrewLogsArgs',\n",
       " 'strict': True,\n",
       " 'parameters': {'description': 'Get the crew logasass.',\n",
       "  'properties': {},\n",
       "  'title': 'GetCrewLogsArgs',\n",
       "  'type': 'object',\n",
       "  'additionalProperties': False,\n",
       "  'required': []},\n",
       " 'description': 'Get the crew logasass.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from arcade.core.agent import generate_function_schema\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class GetCrewLogsArgs(BaseModel):\n",
    "    \"\"\"Get the crew logasass.\"\"\"\n",
    "\n",
    "generate_function_schema(GetCrewLogsArgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution successful: True\n",
      "Output:\n",
      "The sum is 15\n",
      "\n",
      "Variables: {'a': 5, 'b': 10, 'result': 50}\n",
      "\n",
      "Unsafe code execution successful: False\n",
      "Output:\n",
      "Unsafe operations detected\n",
      "\n",
      "Math module code execution successful: True\n",
      "Output:\n",
      "Pi is approximately 3.141592653589793\n",
      "\n",
      "Variables: {'result': 0.7071067811865475}\n"
     ]
    }
   ],
   "source": [
    "# Create a simple code execution sandbox\n",
    "import ast\n",
    "import sys\n",
    "from io import StringIO\n",
    "import traceback\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "\n",
    "class CodeSandbox:\n",
    "    \"\"\"A simple sandbox for executing Python code with safety restrictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_execution_time: int = 5, allowed_modules: Optional[list] = None):\n",
    "        \"\"\"Initialize the sandbox with execution time limits.\n",
    "        \n",
    "        Args:\n",
    "            max_execution_time: Maximum execution time in seconds\n",
    "            allowed_modules: List of module names that are allowed to be imported\n",
    "        \"\"\"\n",
    "        self.max_execution_time = max_execution_time\n",
    "        self.allowed_modules = allowed_modules or ['numpy', 'scipy']\n",
    "        \n",
    "        # Create a restricted globals dictionary with safe builtins\n",
    "        restricted_builtins = {}\n",
    "        safe_builtins = [\n",
    "            'abs', 'all', 'any', 'bool', 'dict', 'dir', 'enumerate', 'filter',\n",
    "            'float', 'format', 'frozenset', 'hash', 'int', 'isinstance', \n",
    "            'issubclass', 'len', 'list', 'map', 'max', 'min', 'print', 'range',\n",
    "            'repr', 'round', 'set', 'slice', 'sorted', 'str', 'sum', 'tuple', 'type',\n",
    "            'zip'\n",
    "        ]\n",
    "        \n",
    "        for builtin in safe_builtins:\n",
    "            if hasattr(__builtins__, builtin):\n",
    "                restricted_builtins[builtin] = getattr(__builtins__, builtin)\n",
    "        \n",
    "        # Set up globals with restricted builtins\n",
    "        self.globals = {'__builtins__': restricted_builtins}\n",
    "        \n",
    "        # Pre-import allowed modules\n",
    "        for module_name in self.allowed_modules:\n",
    "            try:\n",
    "                # Use the built-in __import__ function directly\n",
    "                module = __import__(module_name)\n",
    "                self.globals[module_name] = module\n",
    "            except ImportError:\n",
    "                # Skip if module is not available\n",
    "                pass\n",
    "    \n",
    "    def _is_safe_ast(self, code_ast: ast.AST) -> bool:\n",
    "        \"\"\"Check if the AST contains only safe operations.\n",
    "        \n",
    "        Args:\n",
    "            code_ast: The AST to check\n",
    "            \n",
    "        Returns:\n",
    "            True if the code is safe, False otherwise\n",
    "        \"\"\"\n",
    "        # This is a simplified check - a real sandbox would be more comprehensive\n",
    "        for node in ast.walk(code_ast):\n",
    "            # Check imports against whitelist\n",
    "            if isinstance(node, ast.Import):\n",
    "                for name in node.names:\n",
    "                    if name.name not in self.allowed_modules:\n",
    "                        return False\n",
    "            elif isinstance(node, ast.ImportFrom):\n",
    "                if node.module not in self.allowed_modules:\n",
    "                    return False\n",
    "            # Prevent exec/eval\n",
    "            elif isinstance(node, ast.Call) and isinstance(node.func, ast.Name):\n",
    "                if node.func.id in ['exec', 'eval', 'compile', '__import__']:\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    def execute(self, code: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"Execute code in the sandbox.\n",
    "        \n",
    "        Args:\n",
    "            code: Python code to execute\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (success, output, local_vars)\n",
    "        \"\"\"\n",
    "        # Parse the code to check for unsafe operations\n",
    "        try:\n",
    "            code_ast = ast.parse(code)\n",
    "            if not self._is_safe_ast(code_ast):\n",
    "                return False, \"Unsafe operations detected\", {}\n",
    "        except SyntaxError as e:\n",
    "            return False, f\"Syntax error: {str(e)}\", {}\n",
    "        \n",
    "        # Capture stdout\n",
    "        old_stdout = sys.stdout\n",
    "        captured_output = StringIO()\n",
    "        sys.stdout = captured_output\n",
    "        \n",
    "        # Execute the code\n",
    "        local_vars = {}\n",
    "        success = True\n",
    "        try:\n",
    "            exec(code, self.globals, local_vars)\n",
    "        except Exception as e:\n",
    "            success = False\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            print(traceback.format_exc())\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "        \n",
    "        return success, captured_output.getvalue(), local_vars\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sandbox = CodeSandbox(allowed_modules=['math', 'numpy', 'scipy'])\n",
    "code_to_run = \"\"\"\n",
    "a = 5\n",
    "b = 10\n",
    "print(f\"The sum is {a + b}\")\n",
    "result = a * b\n",
    "\"\"\"\n",
    "\n",
    "success, output, variables = sandbox.execute(code_to_run)\n",
    "print(f\"Execution successful: {success}\")\n",
    "print(f\"Output:\\n{output}\")\n",
    "print(f\"Variables: {variables}\")\n",
    "\n",
    "# Try with unsafe code\n",
    "unsafe_code = \"\"\"\n",
    "import os\n",
    "os.system('echo \"This would be dangerous in a real environment\"')\n",
    "\"\"\"\n",
    "\n",
    "success, output, variables = sandbox.execute(unsafe_code)\n",
    "print(f\"\\nUnsafe code execution successful: {success}\")\n",
    "print(f\"Output:\\n{output}\")\n",
    "\n",
    "# Try with allowed module\n",
    "math_code = \"\"\"\n",
    "print(f\"Pi is approximately {math.pi}\")\n",
    "result = math.sin(math.pi/4)\n",
    "\"\"\"\n",
    "\n",
    "success, output, variables = sandbox.execute(math_code)\n",
    "print(f\"\\nMath module code execution successful: {success}\")\n",
    "print(f\"Output:\\n{output}\")\n",
    "print(f\"Variables: {variables}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution successful: False\n",
      "Output:\n",
      "Error: __import__ not found\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_570748/2193018526.py\", line 96, in execute\n",
      "    exec(code, self.globals, local_vars)\n",
      "  File \"<string>\", line 2, in <module>\n",
      "ImportError: __import__ not found\n",
      "\n",
      "\n",
      "Variables: {}\n"
     ]
    }
   ],
   "source": [
    "thrust_code = \"\"\"\n",
    "import math\n",
    "\n",
    "def calculate_course_correction(current_position, target_position, current_velocity, max_thrust):\n",
    "    # Calculate distance vector (target minus current)\n",
    "    distance_vector = [target_position[i] - current_position[i] for i in range(3)]\n",
    "    \n",
    "    # Calculate distance magnitude\n",
    "    distance = (sum(d**2 for d in distance_vector))**0.5\n",
    "    \n",
    "    # Handle edge case: already at target\n",
    "    if distance < 0.001:\n",
    "        return [0, 0, 0], 0.0\n",
    "    \n",
    "    # Calculate normalized direction vector\n",
    "    direction = [d/distance for d in distance_vector]\n",
    "    \n",
    "    # Calculate dot product of velocity and direction to target\n",
    "    # This tells us if we're moving toward or away from the target\n",
    "    velocity_toward_target = sum(current_velocity[i] * direction[i] for i in range(3))\n",
    "    \n",
    "    # Calculate appropriate thrust power based on distance and velocity\n",
    "    # Use more thrust for greater distances\n",
    "    # Reduce thrust if already moving toward target at good speed\n",
    "    base_thrust = min(1.0, distance / 5000)\n",
    "    \n",
    "    # Adjust thrust based on current velocity\n",
    "    if velocity_toward_target > 0:\n",
    "        # Already moving toward target, reduce thrust\n",
    "        thrust_power = max(0.0, base_thrust - velocity_toward_target / 100)\n",
    "    else:\n",
    "        # Moving away from target, increase thrust\n",
    "        thrust_power = min(1.0, base_thrust - velocity_toward_target / 100)\n",
    "    \n",
    "    return direction, thrust_power\n",
    "\n",
    "print(calculate_course_correction([127, 38, 95, 4], [182, 44, 21, 8], [0, 0, 0], 1.0))\n",
    "\"\"\"\n",
    "\n",
    "success, output, variables = sandbox.execute(thrust_code)\n",
    "print(f\"Execution successful: {success}\")\n",
    "print(f\"Output:\\n{output}\")\n",
    "print(f\"Variables: {variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcade-Rh2i6alR-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
